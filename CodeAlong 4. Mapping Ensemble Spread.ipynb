{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f36736-4af3-4d31-9f6a-a3762d0afd18",
   "metadata": {},
   "source": [
    "# Mapping Ensemble Spread and Statistical Significance\n",
    "## (An Example Using Precipitation)\n",
    "### Authors\n",
    "\n",
    "Samantha Stevenson sstevenson@ucsb.edu\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[Goals](#purpose)\n",
    "\n",
    "[Import Packages](#path)\n",
    "\n",
    "[Pull Data of Interest: Historical Plus SSP](#data_io)\n",
    "\n",
    "[Do Gridpoint Significance Testing](#sigtest)\n",
    "\n",
    "[Map Epoch Differences With Significance Values](#diffmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362328d2-abde-4856-ae91-0c96152cf36c",
   "metadata": {},
   "source": [
    "<a id='purpose'></a> \n",
    "## **Goals**\n",
    "\n",
    "In this tutorial, we will practice some techniques for assessing statistical significance of changes in climate data, and ways to represent significance on map plots!\n",
    "\n",
    "This will allow you to practice skills learned in previous tutorials:\n",
    "- [Mapping Climate Data](https://github.com/climate-datalab/Map-Plots/blob/main/1.%20Mapping%20Climate%20Data.ipynb) (putting spatial data onto a map using Cartopy)\n",
    "- [Plotting Regional Time Series Using Shapefiles](https://github.com/climate-datalab/EnsembleAnalysis/blob/main/2.%20Plotting%20Regional%20Time%20Series%20Using%20Shapefiles.ipynb)  (masking out irregular regions from the climate model grid using shape files)\n",
    "- [Ensemble Spread and Statistical Significance](https://github.com/climate-datalab/EnsembleAnalysis/blob/main/3.%20Ensemble%20Spread%20and%20Statistical%20Significance.ipynb)  (calculating significance of differences)\n",
    "\n",
    "while also learning a new skill that will be presented below:\n",
    "- **Stippling map areas** to show where differences are and are not significant!\n",
    "\n",
    "### **This is the Code-Along version of tutorial 4!!** \n",
    "\n",
    "This notebook contains only a selected subset of the code for tutorial 4, so that it can be completed in a \"code along\" format in a classroom context. If you would like the fully completed version, please see \"4. Mapping Ensemble Spread.ipynb\" in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f3129-4be6-4f34-aa3b-bc2bacb9a5c7",
   "metadata": {},
   "source": [
    "<a id='path'></a> \n",
    "## **Import Packages**\n",
    "\n",
    "As always, we begin by importing the necessary packages for our analysis. We'll be using all the same packages as [tutorial 3 in this repo](https://github.com/climate-datalab/EnsembleAnalysis/blob/main/3.%20Ensemble%20Spread%20and%20Statistical%20Significance.ipynb), including Scipy! As we did previously, we'll import the `scipy.stats` sub-package into our environment as `stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa98061e-1b39-471b-92c2-b6616ac835db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import intake\n",
    "import s3fs\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98698dbb-927f-4e09-8fe9-cc1f84e49b8b",
   "metadata": {},
   "source": [
    "<a id='data_io'></a> \n",
    "## **Pull Data of Interest: Historical Plus SSP**\n",
    "\n",
    "We'll continue working with the CMIP6 data catalog hosted on AWS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65736f3-308c-4545-ad25-4a09e7097563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CMIP6 data catalog, store as a variable\n",
    "catalog = intake.open_esm_datastore('https://cmip6-pds.s3.amazonaws.com/pangeo-cmip6.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea141b-1987-4e6c-9d2a-a744aa830be0",
   "metadata": {},
   "source": [
    "Let's continue our approach from tutorial 3, of identifying ensembles of **historical** and **SSP3-7.0** output! As we discussed there, we'll pull data from the ACCESS-CM2 model, since it has a nice moderate ensemble size (5 members) that will let us get a sense for spread while not taking _that_ long to read in the data.\n",
    "\n",
    "I'll follow the workflow for reading in the data and putting it into an easy-to-use xarray Dataset explained in tutorial 3. Here information for both the historical and future projections is extracted, but ONLY including members that are contained in BOTH projections to make sure we're using comparable sets of data for both time periods. \n",
    "\n",
    "\n",
    "**NOTE:** up till now, all the tutorials relied on surface air temperature (\"tas\") data. Let's try something else! Precipitation is called `pr` in CMIP jargon, and is an atmosphere variable like surface air temperature. We'll stick with monthly data to make the file sizes manageable, which means we'll still be using the `Amon` \"table_id\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16e232a-45b7-48f4-a531-8aec2117ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data just for ACCESS-CM2\n",
    "# Specify search terms to query catalog \n",
    "\n",
    "\n",
    "# Extract historical data\n",
    "\n",
    "# Extract future SSP projection data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9353f6d-a188-4a2b-b375-ae09687c24d8",
   "metadata": {},
   "source": [
    "We can take a quick look at the output data frames `res_access_hist` and `res_access_ssp` to make sure everything looks reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826f928-fbad-47e3-8475-4ec32b77e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d348e-7f89-4fc7-a5e8-d34716e22776",
   "metadata": {},
   "source": [
    "Now we repeat the workflow from tutorial 3. First, list the unique ensemble members that exist in each of the historical and SSP ensembles, then find the members which are common between them. For this particular ensemble, the members happen to be identical, but going through this exercise is important since that's often not the case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72d2f9f4-e5ac-4729-8f1c-0ceb63053daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the unique ensemble members\n",
    "# historical\n",
    "\n",
    "# SSP\n",
    "\n",
    "# Convert these to sets, find the intersection between them, and convert back to a list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb3846-21b5-4581-80b4-ad328e206c74",
   "metadata": {},
   "source": [
    "Next, we loop over each of the unique member ids, and do the following:\n",
    "\n",
    "1) Locate the member id in the historical and SSP data frames;\n",
    "\n",
    "2) Read in the historical and SSP data associated with that member from both data frames;\n",
    "\n",
    "3) Concatenate the historical/SSP data in time, to get a continuous time series for that member;\n",
    "\n",
    "4) Add the resulting xarray Dataset to a list of output data.\n",
    "\n",
    "After the loop is complete, we then use `xr.concat` to make the whole thing into a nice package with a new dimension called `member`, and store the member names as the coordinate information for that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82360c1-6c4f-4f88-ad10-e27ef8e70d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list for output data\n",
    "\n",
    "# Retrieve number of members the historical and SSP ensembles have in common\n",
    "\n",
    "# Loop over all members-in-common\n",
    "for mem in range(num):\n",
    "    # Print statement to keep track of which member we're working on\n",
    "    \n",
    "    # Figure out where this member is in the historical ensemble\n",
    "    # True/False array showing whether or not the member_id matches our member of interest\n",
    "    \n",
    "    # extract (first) location where the mask is True\n",
    "    \n",
    "    \n",
    "    # Do the same thing for the SSP ensemble\n",
    "    \n",
    "    \n",
    "    # Extract data from each entry as xarray\n",
    "    \n",
    "    \n",
    "    # Concatenate the historical and SSP data across the time dimension\n",
    "    \n",
    "    # Add the concatenated data to a list\n",
    "\n",
    "# Concatenate the list into a single xarray object\n",
    "\n",
    "# Store the actual member information as values of the new dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05246ad-f775-4587-885a-8ac4ed51d0b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### **Convert units to make the results more intuitive**\n",
    "\n",
    "Before we get into the more sophisticated calculations, let's take a minute to think about the _units_ of the data. You can display them either by printing out the entire `pr` variable and scrolling down to view the units, or by using this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d93eb5f0-0da8-4453-8fc2-e6ca01a146c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print out units of precip data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc20db-8c10-40f4-9136-61eecc273614",
   "metadata": {},
   "source": [
    "These are standard units for precipitation, but are the total _mass flux_ of water and not something that people intuitively understand like mm or inches.\n",
    "\n",
    "Instead, let's convert this to a more intuitive unit: mm/day! The conversion factor for this is:\n",
    "\n",
    "mm/day = kg/(m^2 s) x (1 m^3/1000 kg) x (1000 mm/1 m) x (86400 s/day)\n",
    "\n",
    "where the 1000 kg/m^3 is the density of water, and the seconds/day number comes from multiplying 24 hours x 60 minutes/hour x 60 seconds/minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986933f7-b5c3-4e31-b79c-92a25a3acfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert kg/(m^2 s) to mm/day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641bdad-4620-43df-b9be-bdd246471b04",
   "metadata": {},
   "source": [
    "<a id='sigtest'></a> \n",
    "## **Do Gridpoint Significance Testing**\n",
    "\n",
    "Now, we are NOT going to perform the regional averaging steps we used in tutorial 3! We're asking a different question here:\n",
    "\n",
    "_How significant are changes in average precipitation between two time periods, for every place on Earth?_\n",
    "\n",
    "First, we need to define time periods of interest, over which to do our differencing. Let's choose the early period to be 1850-1900, and the late period to be 2050-2100; you can change this to whatever you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660cd1a9-abb5-4b3b-baf2-b3a12280607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for an earlier period: 20th century\n",
    "\n",
    "# Get data for a later period: 21st century\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda34675-9ec8-408c-9573-51baf5bbc8df",
   "metadata": {},
   "source": [
    "Because we're interested in finding the difference in the _time-averaged_ precipitation between the two periods, we then take the mean over the time dimension for both epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f89c3d-bc92-4a89-8234-0b17e4080591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the time average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ce988-7614-4329-b054-f5d0589e0e7b",
   "metadata": {},
   "source": [
    "Now comes the slow part! You don't have to do it this way, but I find it more efficient to extract the data from both periods into Numpy arrays, so that we can quickly iterate over each lat and lon point to apply our statistical test.\n",
    "\n",
    "**Depending on your machine, this may take 3-5 minutes or more; be patient!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6dcf48-5665-478b-9c12-925d56d8c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values into Numpy arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a0c1b-bcc0-4979-88b5-3b721c885408",
   "metadata": {},
   "source": [
    "The good news is that once you've done the extraction, the rest should go quite quickly!  \n",
    "\n",
    "We can loop through each combination of latitude and longitude, then calculate the significance of differences between our two samples at every point. This will come in handy in a couple minutes when we make our plots of epoch differences!\n",
    "\n",
    "The code block below does a couple of things:\n",
    "\n",
    "1) Defines empty output Numpy arrays, of size [lat x lon] where the lat and lon dimensions are the same as the input precipitation data array;\n",
    "\n",
    "2) Loops over lat/lon points, and extracts the p value associated with the statistical test of interest (here I'm doing BOTH the T-test AND the rank-sum test as a demonstration, but usually you'll only want to do one or the other); and\n",
    "\n",
    "3) Saves the output p value in the output array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6e615b0-39af-404c-a065-c54b3b2668a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty output arrays\n",
    "# T-test\n",
    "\n",
    "# Rank-sum test\n",
    "\n",
    "# Loop over lat, lon in the Numpy array\n",
    "for latidx in range(len(data_early_mn.lat)):\n",
    "    for lonidx in range(len(data_early_mn.lon)):\n",
    "        # Run test on each grid point\n",
    "        # T-test\n",
    "        \n",
    "        # Rank-sum\n",
    "        \n",
    "        \n",
    "        # Save results in the output array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cdc63-6392-4cba-b62a-1ceccba4b2d2",
   "metadata": {},
   "source": [
    "<a id='diffmaps'></a> \n",
    "## **Map Epoch Differences With Significance Values**\n",
    "\n",
    "OK now we're ready to display some results! Let's make a global map showing the epoch differences in precipitation between the late and early periods, following the same steps as we used back in the [mapping tutorial](https://github.com/climate-datalab/Map-Plots/tree/main) section. The code block below is taken nearly directly from that tutorial, so check there if you need more explanation.\n",
    "\n",
    "To make things run faster, we can use the Numpy versions of the data rather than xarray! But now comes the new step - we're going to take the p-value array we created earlier, and identify all the **locations** in that array where the value is less than some threshold. There's some flexibility in this choice, but typical choices are:\n",
    "- p-value = 0.1: 90 percent confidence\n",
    "- p-value = 0.05: 95 percent confidence\n",
    "- p-value = 0.01: 99 percent confidence\n",
    "\n",
    "Once we have the locations of the array points that we consider significant, the code block below then puts a dot on the map at each location!\n",
    "\n",
    "**note: here I'm using the rank-sum test for significance; you can also do the same thing for the T test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7735576d-507a-4a1e-b796-3201ff32bcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate ensemble-mean epoch difference\n",
    "\n",
    "# Define object containing PlateCarree projection\n",
    "\n",
    "# Create figure/axis objects, use the map object to specify associated projection\n",
    "\n",
    "# Plot temperature data on the axes using the coolwarm colormap\n",
    "\n",
    "\n",
    "# Add colorbar and label it\n",
    "\n",
    "# Add coastline/border lines\n",
    "\n",
    "# Add grid lines\n",
    "\n",
    "\n",
    "# Set font size for x, y-axis labels\n",
    "\n",
    "\n",
    "# Statistical significance for historical vs future precip change regressed on gradient change\n",
    "\n",
    "# Add title, show plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f210d-42d1-41e3-8a1a-e74a07de4b90",
   "metadata": {},
   "source": [
    "There you have it! Feel free to customize this plot as you like: you can change the significance threshold, stipple the INSIGNIFICANT points instead of the significant ones, or use a different significance test if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad1f2c1a-55e5-4c7d-8b91-f1343eb7f470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564cecf8-1f24-451d-912f-86c30461e27b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
